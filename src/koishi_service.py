# koishi_service.py
import socket
import uvicorn
from fastapi import FastAPI, Request
from .ai_chat_system import AIChatSystem
import time
import json
from fastapi.responses import StreamingResponse
# ç¡®ä¿æ­£ç¡®å¯¼å…¥ colorama
from colorama import Fore, init
from .database import DatabaseManager

init(autoreset=True)

app = FastAPI()

chat_system = AIChatSystem()
chat_system.db = DatabaseManager()


@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    try:
        data = await request.json()
        print(Fore.CYAN + f"æ”¶åˆ°è¯·æ±‚: {data}")

        # åŠ¨æ€é€‰æ‹©æ¨¡å‹ï¼Œåç«¯æ”¯æŒ deepseek-chat / deepseek-vl / o4-mini-preview
        selected_model = data.get("model", "deepseek-chat")

        # æ–°å¢ï¼šneko æ¨¡å‹ä¸“å±å¤„ç†ï¼Œç›´æ¥èµ°æœ¬åœ° AIChatSystem
        if selected_model == "neko":
            user_input = ""
            image_data = None
            for msg in reversed(data.get("messages", [])):
                if msg.get("role") == "user":
                    content = msg.get("content", "")
                    if isinstance(content, list):
                        for item in content:
                            if isinstance(item, dict) and item.get("type") == "text":
                                user_input += item.get("text", "")
                            elif isinstance(item, dict) and item.get("type") == "image_url":
                                image_data = item.get("image_url", {}).get("url")
                    else:
                        user_input = content
                    break

            response_text = chat_system.chat(user_input, image=image_data)
            return {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": "neko",
                "choices": [{
                    "index": 0,
                    "message": {"role": "assistant", "content": response_text},
                    "finish_reason": "stop"
                }],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
            }

        # æå–ç”¨æˆ·æ¶ˆæ¯
        user_input = ""
        messages = data.get('messages', [])
        for msg in reversed(messages):
            if msg.get('role') == 'user':
                user_input = msg.get('content', "")
                break

        print(Fore.GREEN + f"ç”¨æˆ·è¾“å…¥: {user_input}")

        stream_mode = data.get("stream", False)
        if stream_mode:
            async def event_generator():
                content_accum = ""
                # é€å—è¯·æ±‚ API å¹¶æ¨é€
                for chunk in chat_system.client.chat.completions.create(
                        model=selected_model,
                        messages=chat_system.messages + [{"role": "user", "content": user_input}],
                        temperature=0.7, max_tokens=200, timeout=30, stream=True
                ):
                    # ä¿®æ”¹è¿™é‡Œï¼Œä»å±æ€§è¯»å– content
                    delta = getattr(chunk.choices[0].delta, "content", "")
                    if delta:
                        content_accum += delta
                        payload = {
                            "choices": [{
                                "delta": {"content": delta},
                                "index": 0,
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(payload)}\n\n"
                chat_system.db.save_chat(user_input, content_accum)
                yield "data: [DONE]\n\n"

            return StreamingResponse(event_generator(), media_type="text/event-stream")

        # è°ƒç”¨ DeepSeek æ¥å£ï¼Œä½¿ç”¨åŠ¨æ€æ¨¡å‹
        response = chat_system.client.chat.completions.create(
            model=selected_model,
            messages=chat_system.messages + [{"role": "user", "content": user_input}],
            temperature=0.7,
            max_tokens=200,
            timeout=30
        )
        ai_response = response.choices[0].message.content
        chat_system.messages.append({"role": "assistant", "content": ai_response})
        chat_system.db.save_chat(user_input, ai_response)

        # æå– usage ä¿¡æ¯
        usage_info = getattr(response, "usage", None)
        result = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": selected_model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": ai_response
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": usage_info.prompt_tokens if usage_info else 0,
                "completion_tokens": usage_info.completion_tokens if usage_info else 0,
                "total_tokens": usage_info.total_tokens if usage_info else 0
            }
        }

        print(Fore.CYAN + f"å‘é€å“åº”: {result}")
        return result

    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(Fore.RED + f"å®Œæ•´é”™è¯¯ä¿¡æ¯:\n{error_trace}")

        # å³ä½¿å‡ºç°é”™è¯¯ï¼Œä¹Ÿè¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼
        return {
            "id": f"error-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": data.get("model", "deepseek-chat"),
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": f"å‡ºé”™äº†å–µ({str(e)})"
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }


@app.get("/")
async def root():
    """æ ¹è·¯å¾„è¿”å›æœåŠ¡ä¿¡æ¯"""
    return {
        "service": "Koishi API Service",
        "endpoints": [
            "/v1/chat/completions (POST)",
            "/v1/models (GET)",
            "/health (GET)"
        ]
    }


@app.get("/v1/models")
async def model_list():
    """è¿”å›æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
    return {
        "object": "list",
        "data": [
            {"id": "deepseek-chat", "object": "model", "created": int(time.time()), "owned_by": "local"},
            {"id": "deepseek-v1", "object": "model", "created": int(time.time()), "owned_by": "local"},
            {"id": "neko", "object": "model", "created": int(time.time()), "owned_by": "neko"}  # æ·»åŠ nekoæ¨¡å‹
        ]
    }


@app.get("/health")
async def health_check():
    """æœåŠ¡å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "service": "Koishi API"}


def is_port_in_use(port: int) -> bool:
    """æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)  # è®¾ç½®è¶…æ—¶æ—¶é—´
        try:
            s.connect(("localhost", port))
            return True
        except (socket.timeout, ConnectionRefusedError):
            return False
        except OSError:
            # å…¶ä»–OSErrorä¹Ÿè®¤ä¸ºç«¯å£ä¸å¯ç”¨
            return True


def find_available_port(start_port=5000, end_port=5100):
    """åœ¨æŒ‡å®šèŒƒå›´å†…æŸ¥æ‰¾å¯ç”¨ç«¯å£"""
    for port in range(start_port, end_port + 1):
        if not is_port_in_use(port):
            return port
    return None


def run_koishi_service():
    """Koishiæ˜ å°„æ¨¡å¼ (FastAPIæœåŠ¡)"""
    # åˆ›å»ºFastAPIåº”ç”¨
    fastapi_app = FastAPI()

    # æ·»åŠ CORSä¸­é—´ä»¶
    from fastapi.middleware.cors import CORSMiddleware
    fastapi_app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    chat_system = AIChatSystem()
    chat_system.db = DatabaseManager()

    @fastapi_app.post("/v1/chat/completions")
    async def openai_api(request: Request):
        try:
            data = await request.json()
            print(Fore.CYAN + f"æ”¶åˆ°è¯·æ±‚: {data}")

            # åŠ¨æ€é€‰æ‹©æ¨¡å‹ï¼Œåç«¯æ”¯æŒ deepseek-chat / deepseek-vl / o4-mini-preview
            selected_model = data.get("model", "deepseek-chat")

            # æ–°å¢ï¼šneko æ¨¡å‹ä¸“å±å¤„ç†
            if selected_model == "neko":
                user_input = ""
                image_data = None
                for msg in reversed(data.get("messages", [])):
                    if msg.get("role") == "user":
                        content = msg.get("content", "")
                        if isinstance(content, list):
                            for item in content:
                                if isinstance(item, dict) and item.get("type") == "text":
                                    user_input += item.get("text", "")
                                elif isinstance(item, dict) and item.get("type") == "image_url":
                                    image_data = item.get("image_url", {}).get("url")
                        else:
                            user_input = content
                        break

                response_text = chat_system.chat(user_input, image=image_data)
                return {
                    "id": f"chatcmpl-{int(time.time())}",
                    "object": "chat.completion",
                    "created": int(time.time()),
                    "model": "neko",
                    "choices": [{
                        "index": 0,
                        "message": {"role": "assistant", "content": response_text},
                        "finish_reason": "stop"
                    }],
                    "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
                }

            # æå–ç”¨æˆ·æ¶ˆæ¯
            user_input = ""
            messages = data.get('messages', [])
            for msg in reversed(messages):
                if msg.get('role') == 'user':
                    user_input = msg.get('content', "")
                    break
            print(Fore.GREEN + f"ç”¨æˆ·è¾“å…¥: {user_input}")

            stream_mode = data.get("stream", False)
            if stream_mode:
                async def event_generator():
                    content_accum = ""
                    # é€å—è¯·æ±‚ API å¹¶æ¨é€
                    for chunk in chat_system.client.chat.completions.create(
                            model=selected_model,
                            messages=chat_system.messages + [{"role": "user", "content": user_input}],
                            temperature=0.7, max_tokens=200, timeout=30, stream=True
                    ):
                        # ä¿®æ”¹è¿™é‡Œï¼Œä»å±æ€§è¯»å– content
                        delta = getattr(chunk.choices[0].delta, "content", "")
                        if delta:
                            content_accum += delta
                            payload = {
                                "choices": [{
                                    "delta": {"content": delta},
                                    "index": 0,
                                    "finish_reason": None
                                }]
                            }
                            yield f"data: {json.dumps(payload)}\n\n"
                    chat_system.db.save_chat(user_input, content_accum)
                    yield "data: [DONE]\n\n"

                return StreamingResponse(event_generator(), media_type="text/event-stream")

            # è°ƒç”¨ DeepSeek æ¥å£ï¼Œä½¿ç”¨åŠ¨æ€æ¨¡å‹
            response = chat_system.client.chat.completions.create(
                model=selected_model,
                messages=chat_system.messages + [{"role": "user", "content": user_input}],
                temperature=0.7,
                max_tokens=200,
                timeout=30
            )
            ai_response = response.choices[0].message.content
            chat_system.messages.append({"role": "assistant", "content": ai_response})
            chat_system.db.save_chat(user_input, ai_response)

            usage_info = getattr(response, "usage", None)
            result = {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": selected_model,
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": ai_response
                        },
                        "finish_reason": "stop"
                    }
                ],
                "usage": {
                    "prompt_tokens": usage_info.prompt_tokens if usage_info else 0,
                    "completion_tokens": usage_info.completion_tokens if usage_info else 0,
                    "total_tokens": usage_info.total_tokens if usage_info else 0
                }
            }
            print(Fore.CYAN + f"å‘é€å“åº”: {result}")
            return result

        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            print(Fore.RED + f"å®Œæ•´é”™è¯¯ä¿¡æ¯:\n{error_trace}")
            return {
                "id": f"error-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": data.get("model", "deepseek-chat"),
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": f"å‡ºé”™äº†å–µ({str(e)})"
                        },
                        "finish_reason": "stop"
                    }
                ],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
            }

    @fastapi_app.get("/")
    async def root():
        """æ ¹è·¯å¾„è¿”å›æœåŠ¡ä¿¡æ¯"""
        return {
            "service": "Koishi API Service",
            "endpoints": [
                "/v1/chat/completions (POST)",
                "/v1/models (GET)",
                "/health (GET)"
            ]
        }

    @fastapi_app.get("/v1/models")
    async def model_list():
        """è¿”å›æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        return {
            "object": "list",
            "data": [
                {"id": "deepseek-chat", "object": "model", "created": int(time.time()), "owned_by": "deepseek"},
                {"id": "neko", "object": "model", "created": int(time.time()), "owned_by": "neko"},  # æ·»åŠ nekoæ¨¡å‹
                {"id": "gpt-3.5-turbo", "object": "model", "created": int(time.time()), "owned_by": "neko"}
            ]
        }

    @fastapi_app.get("/health")
    async def health_check():
        """æœåŠ¡å¥åº·æ£€æŸ¥"""
        return {"status": "ok", "service": "Koishi API"}

    # ç»Ÿä¸€APIæ¥å£ï¼Œéšè—åç«¯å¤šä¸ªAPIçš„å¤æ‚æ€§
    @fastapi_app.post("/v1/unified/chat/completions")
    async def unified_chat_completions(request: Request):
        """
        ç»Ÿä¸€èŠå¤©å®Œæˆæ¥å£
        å‰ç«¯å¯ä»¥åƒè°ƒç”¨å•ä¸€æ¨¡å‹ä¸€æ ·è°ƒç”¨æ­¤æ¥å£ï¼Œ
        åç«¯ä¼šæ ¹æ®å†…å®¹è‡ªåŠ¨å†³å®šè°ƒç”¨å“ªäº›APIï¼ˆæ–‡æœ¬ã€å›¾åƒè¯†åˆ«ã€ç½‘ç»œæœç´¢ç­‰ï¼‰
        """
        try:
            data = await request.json()
            print(Fore.CYAN + f"æ”¶åˆ°ç»Ÿä¸€APIè¯·æ±‚: {data}")

            # æå–ç”¨æˆ·æ¶ˆæ¯
            user_input = ""
            image_urls = []
            messages = data.get('messages', [])
            
            # éå†æ‰€æœ‰æ¶ˆæ¯ï¼Œæå–æ–‡æœ¬å’Œå›¾åƒURL
            for msg in messages:
                if msg.get('role') == 'user':
                    content = msg.get('content', "")
                    # å¤„ç†åŒ…å«image_urlçš„æ¶ˆæ¯
                    if isinstance(content, list):
                        # å¦‚æœæ˜¯åˆ—è¡¨ç±»å‹ï¼Œè¯´æ˜åŒ…å«å¤šç§ç±»å‹çš„å†…å®¹ï¼ˆå¦‚textå’Œimage_urlï¼‰
                        text_parts = []
                        for item in content:
                            if isinstance(item, dict):
                                if item.get('type') == 'text':
                                    text_parts.append(item.get('text', ''))
                                elif item.get('type') == 'image_url':
                                    # å¯¹äºå›¾ç‰‡URLï¼Œæå–å®é™…URLå¹¶ä¿å­˜
                                    image_url = item.get('image_url', {}).get('url', '')
                                    if image_url:
                                        image_urls.append(image_url)
                                        text_parts.append(f'[å›¾ç‰‡: {image_url}]')
                        user_input += ''.join(text_parts)
                    else:
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨
                        user_input += content

            print(Fore.GREEN + f"å¤„ç†åç”¨æˆ·è¾“å…¥: {user_input}")
            print(Fore.GREEN + f"æå–åˆ°å›¾ç‰‡URL: {image_urls}")

            stream_mode = data.get("stream", False)
            if stream_mode:
                async def event_generator():
                    # è°ƒç”¨AIèŠå¤©ç³»ç»Ÿå¤„ç†ï¼ˆä¼šè‡ªåŠ¨å¤„ç†å›¾ç‰‡å’Œæœç´¢ç­‰ï¼‰
                    # å¯¹äºæµå¼å“åº”ï¼Œæˆ‘ä»¬å…ˆç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„å›å¤ï¼Œç„¶åé€å­—å‘é€
                    if image_urls:
                        # å¦‚æœæœ‰å›¾ç‰‡URLï¼Œä¼ é€’ç¬¬ä¸€å¼ å›¾ç‰‡
                        full_response = chat_system.chat(user_input, image=image_urls[0])
                    else:
                        # å¦åˆ™åªå¤„ç†æ–‡æœ¬
                        full_response = chat_system.chat(user_input)
                    
                    # é€å­—å‘é€å“åº”
                    for i, char in enumerate(full_response):
                        payload = {
                            "choices": [{
                                "delta": {"content": char},
                                "index": 0,
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(payload)}\n\n"
                    
                    # å‘é€ç»“æŸæ ‡è®°
                    yield "data: [DONE]\n\n"

                return StreamingResponse(event_generator(), media_type="text/event-stream")

            # è°ƒç”¨AIèŠå¤©ç³»ç»Ÿå¤„ç†ï¼ˆä¼šè‡ªåŠ¨å¤„ç†å›¾ç‰‡å’Œæœç´¢ç­‰ï¼‰
            if image_urls:
                # å¦‚æœæœ‰å›¾ç‰‡URLï¼Œä¼ é€’ç¬¬ä¸€å¼ å›¾ç‰‡
                response_text = chat_system.chat(user_input, image=image_urls[0])
            else:
                # å¦åˆ™åªå¤„ç†æ–‡æœ¬
                response_text = chat_system.chat(user_input)

            # æ„é€ ç¬¦åˆOpenAIæ ¼å¼çš„å“åº”
            result = {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": "neko",  # å¯¹å‰ç«¯éšè—çœŸå®æ¨¡å‹ï¼Œä½¿ç”¨"neko"ä½œä¸ºæ¨¡å‹åç§°
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": response_text
                        },
                        "finish_reason": "stop"
                    }
                ],
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0
                }
            }
            
            print(Fore.CYAN + f"å‘é€ç»Ÿä¸€APIå“åº”: {result}")
            return result

        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            print(Fore.RED + f"ç»Ÿä¸€APIé”™è¯¯:\n{error_trace}")
            
            # è¿”å›é”™è¯¯ä¿¡æ¯ä½†ä»ä¿æŒOpenAIæ ¼å¼
            return {
                "id": f"error-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": "neko",  # é”™è¯¯å“åº”ä¹Ÿä½¿ç”¨"neko"ä½œä¸ºæ¨¡å‹åç§°
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": f"å‡ºé”™äº†å–µ({str(e)})"
                        },
                        "finish_reason": "stop"
                    }
                ],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
            }

    # æŸ¥æ‰¾å¯ç”¨ç«¯å£
    port = find_available_port()
    if port is None:
        print(Fore.RED + "é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°å¯ç”¨ç«¯å£ (5000-5100)")
        return

    print(Fore.CYAN + f"\nğŸš€ Koishiæ˜ å°„æ¨¡å¼å·²å¯åŠ¨: http://localhost:{port}/v1")
    print(Fore.YELLOW + f"è¯·åœ¨ AstrBot ä¸­å°† API åœ°å€è®¾ç½®ä¸º: http://localhost:{port}/v1")
    # å¢åŠ è¶…æ—¶è®¾ç½®å’Œå“åº”å¤´é…ç½®
    uvicorn.run(
        fastapi_app,
        host="127.0.0.1",  # ä½¿ç”¨127.0.0.1è€Œä¸æ˜¯0.0.0.0æ›´å®‰å…¨
        port=port,  # ä½¿ç”¨æ‰¾åˆ°çš„å¯ç”¨ç«¯å£
        timeout_keep_alive=120,  # å¢åŠ ä¿æŒè¿æ¥è¶…æ—¶
        log_level="debug"  # å¼€å¯è°ƒè¯•æ—¥å¿—
    )
